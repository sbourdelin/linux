/* drivers/misc/lowmemorykiller.c
 *
 * The lowmemorykiller driver lets user-space specify a set of memory thresholds
 * where processes with a range of oom_score_adj values will get killed. Specify
 * the minimum oom_score_adj values in
 * /sys/module/lowmemorykiller/parameters/adj and the number of free pages in
 * /sys/module/lowmemorykiller/parameters/minfree. Both files take a comma
 * separated list of numbers in ascending order.
 *
 * For example, write "0,8" to /sys/module/lowmemorykiller/parameters/adj and
 * "1024,4096" to /sys/module/lowmemorykiller/parameters/minfree to kill
 * processes with a oom_score_adj value of 8 or higher when the free memory
 * drops below 4096 pages and kill processes with a oom_score_adj value of 0 or
 * higher when the free memory drops below 1024 pages.
 *
 * The driver considers memory used for caches to be free, but if a large
 * percentage of the cached memory is locked this can be very inaccurate
 * and processes may not get killed until the normal oom killer is triggered.
 *
 * Copyright (C) 2007-2008 Google, Inc.
 *
 * This software is licensed under the terms of the GNU General Public
 * License version 2, as published by the Free Software Foundation, and
 * may be copied, distributed, and modified under those terms.
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 */

#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt

#include <linux/init.h>
#include <linux/moduleparam.h>
#include <linux/kernel.h>
#include <linux/mm.h>
#include <linux/oom.h>
#include <linux/sched.h>
#include <linux/swap.h>
#include <linux/rcupdate.h>
#include <linux/profile.h>
#include <linux/slab.h>
#include <linux/notifier.h>
#include <linux/oom_score_notifier.h>
#include "lowmemorykiller.h"
#include "lowmemorykiller_stats.h"
#include "lowmemorykiller_tasks.h"

u32 lowmem_debug_level = 1;
static short lowmem_adj[6] = {
	0,
	1,
	6,
	12,
};

static int lowmem_adj_size = 4;
static int lowmem_minfree[6] = {
	3 * 512,	/* 6MB */
	2 * 1024,	/* 8MB */
	4 * 1024,	/* 16MB */
	16 * 1024,	/* 64MB */
};

static int lowmem_minfree_size = 4;

struct calculated_params {
	long selected_tasksize;
	long minfree;
	int other_file;
	int other_free;
	int dynamic_max_queue_len;
	short selected_oom_score_adj;
	short min_score_adj;
};

static int kill_needed(int level, struct shrink_control *sc,
		       struct calculated_params *cp)
{
	int i;
	int array_size = ARRAY_SIZE(lowmem_adj);

	cp->other_free = global_page_state(NR_FREE_PAGES) - totalreserve_pages;
	cp->other_file = global_page_state(NR_FILE_PAGES) -
		global_page_state(NR_SHMEM) -
		global_page_state(NR_UNEVICTABLE) -
		total_swapcache_pages();

	cp->minfree = 0;
	cp->min_score_adj = OOM_SCORE_ADJ_MAX;
	if (lowmem_adj_size < array_size)
		array_size = lowmem_adj_size;
	if (lowmem_minfree_size < array_size)
		array_size = lowmem_minfree_size;
	for (i = 0; i < array_size; i++) {
		cp->minfree = lowmem_minfree[i];
		if (cp->other_free < cp->minfree &&
		    cp->other_file < cp->minfree) {
			cp->min_score_adj = lowmem_adj[i];
			break;
		}
	}
	if (sc->nr_to_scan > 0)
		lowmem_print(3, "lowmem_shrink %lu, %x, ofree %d %d, ma %hd\n",
			     sc->nr_to_scan, sc->gfp_mask, cp->other_free,
			     cp->other_file, cp->min_score_adj);
	cp->dynamic_max_queue_len = array_size - i + 1;
	cp->selected_oom_score_adj = level;
	if (level >= cp->min_score_adj)
		return 1;

	return 0;
}

static void print_obituary(struct task_struct *doomed,
			   struct calculated_params *cp,
			   struct shrink_control *sc) {
	long cache_size = cp->other_file * (long)(PAGE_SIZE / 1024);
	long cache_limit = cp->minfree * (long)(PAGE_SIZE / 1024);
	long free = cp->other_free * (long)(PAGE_SIZE / 1024);

	lowmem_print(1, "Killing '%s' (%d), adj %hd,\n"
		     "   to free %ldkB on behalf of '%s' (%d) because\n"
		     "   cache %ldkB is below limit %ldkB for oom_score_adj %hd\n"
		     "   Free memory is %ldkB above reserved.\n"
		     "   Free CMA is %ldkB\n"
		     "   Total reserve is %ldkB\n"
		     "   Total free pages is %ldkB\n"
		     "   Total file cache is %ldkB\n"
		     "   Slab Reclaimable is %ldkB\n"
		     "   Slab UnReclaimable is %ldkB\n"
		     "   Total Slab is %ldkB\n"
		     "   GFP mask is 0x%x\n"
		     "   queue len is %d of max %d\n",
		     doomed->comm, doomed->pid,
		     cp->selected_oom_score_adj,
		     cp->selected_tasksize * (long)(PAGE_SIZE / 1024),
		     current->comm, current->pid,
		     cache_size, cache_limit,
		     cp->min_score_adj,
		     free,
		     global_page_state(NR_FREE_CMA_PAGES) *
		     (long)(PAGE_SIZE / 1024),
		     totalreserve_pages * (long)(PAGE_SIZE / 1024),
		     global_page_state(NR_FREE_PAGES) *
		     (long)(PAGE_SIZE / 1024),
		     global_page_state(NR_FILE_PAGES) *
		     (long)(PAGE_SIZE / 1024),
		     global_page_state(NR_SLAB_RECLAIMABLE) *
		     (long)(PAGE_SIZE / 1024),
		     global_page_state(NR_SLAB_UNRECLAIMABLE) *
		     (long)(PAGE_SIZE / 1024),
		     global_page_state(NR_SLAB_RECLAIMABLE) *
		     (long)(PAGE_SIZE / 1024) +
		     global_page_state(NR_SLAB_UNRECLAIMABLE) *
		     (long)(PAGE_SIZE / 1024),
		     sc->gfp_mask,
		     death_pending_len,
		     cp->dynamic_max_queue_len);
}

static unsigned long lowmem_count(struct shrinker *s,
				  struct shrink_control *sc)
{
	struct lmk_rb_watch *lrw;
	struct calculated_params cp;
	short score;

	lmk_inc_stats(LMK_COUNT);
	cp.selected_tasksize = 0;
	spin_lock(&lmk_task_lock);
	lrw = __lmk_first();
	if (lrw && lrw->tsk->mm) {
		int rss = get_mm_rss(lrw->tsk->mm);

		score = lrw->tsk->signal->oom_score_adj;
		spin_unlock(&lmk_task_lock);
		if (kill_needed(score, sc, &cp))
			if (death_pending_len < cp.dynamic_max_queue_len)
				cp.selected_tasksize = rss;

	} else {
		spin_unlock(&lmk_task_lock);
	}

	return cp.selected_tasksize;
}


static unsigned long lowmem_scan(struct shrinker *s, struct shrink_control *sc)
{
	struct task_struct *selected = NULL;
	unsigned long nr_to_scan = sc->nr_to_scan;
	struct lmk_rb_watch *lrw;
	int do_kill;
	struct calculated_params cp;

	lmk_inc_stats(LMK_SCAN);

	cp.selected_tasksize = 0;
	spin_lock(&lmk_task_lock);

	lrw = __lmk_first();
	if (lrw) {
		if (lrw->tsk->mm) {
			cp.selected_tasksize = get_mm_rss(lrw->tsk->mm);
		} else {
			lowmem_print(1, "pid:%d no mem\n", lrw->tsk->pid);
			lmk_inc_stats(LMK_ERROR);
			goto unlock_out;
		}

		do_kill = kill_needed(lrw->key, sc, &cp);

		if (death_pending_len >= cp.dynamic_max_queue_len) {
			lmk_inc_stats(LMK_BUSY);
			goto unlock_out;
		}

		if (do_kill) {
			struct lmk_death_pending_entry *ldpt;

			selected = lrw->tsk;

			/* there is a chance that task is locked,
			 * and the case where it locked in oom_score_adj_write
			 * we might have deadlock. There is no macro for it
			 *  and this is the only place there is a try on
			 * the task_lock.
			 */
			if (!spin_trylock(&selected->alloc_lock)) {
				lmk_inc_stats(LMK_ERROR);
				lowmem_print(1, "Failed to lock task.\n");
				lmk_inc_stats(LMK_BUSY);
				goto unlock_out;
			}

			/* move to kill pending set */
			ldpt = kmem_cache_alloc(lmk_dp_cache, GFP_ATOMIC);
			ldpt->tsk = selected;

			__lmk_death_pending_add(ldpt);
			if (!__lmk_task_remove(selected, lrw->key))
				WARN_ON(1);

			spin_unlock(&lmk_task_lock);

			set_tsk_thread_flag(selected, TIF_MEMDIE);
			send_sig(SIGKILL, selected, 0);
			task_set_lmk_waiting(selected);

			print_obituary(selected, &cp, sc);

			task_unlock(selected);
			lmk_inc_stats(LMK_KILL);
			goto out;
		} else {
			lmk_inc_stats(LMK_WASTE);
		}
	} else {
		lmk_inc_stats(LMK_NO_KILL);
	}
unlock_out:
	cp.selected_tasksize = SHRINK_STOP;
	spin_unlock(&lmk_task_lock);
out:
	if (cp.selected_tasksize == 0)
		lowmem_print(2, "list empty nothing to free\n");
	lowmem_print(4, "lowmem_shrink %lu, %x, return %ld\n",
		     nr_to_scan, sc->gfp_mask, cp.selected_tasksize);

	return cp.selected_tasksize;
}

static struct shrinker lowmem_shrinker = {
	.scan_objects = lowmem_scan,
	.count_objects = lowmem_count,
	.seeks = DEFAULT_SEEKS * 16
};

static int __init lowmem_init(void)
{
	lmk_dp_cache = KMEM_CACHE(lmk_death_pending_entry, 0);
	lmk_task_cache = KMEM_CACHE(lmk_rb_watch, 0);
	oom_score_notifier_register(&lmk_oom_score_nb);
	register_shrinker(&lowmem_shrinker);
	init_procfs_lmk();
	return 0;
}
device_initcall(lowmem_init);

/*
 * not really modular, but the easiest way to keep compat with existing
 * bootargs behaviour is to continue using module_param here.
 */
module_param_named(cost, lowmem_shrinker.seeks, int, 0644);
module_param_array_named(adj, lowmem_adj, short, &lowmem_adj_size, 0644);
module_param_array_named(minfree, lowmem_minfree, uint, &lowmem_minfree_size,
			 0644);
module_param_named(debug_level, lowmem_debug_level, uint, 0644);

